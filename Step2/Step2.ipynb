{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the command below to start a training job using the attention configuration. --run_dir is the directory where checkpoints and TensorBoard data for this run will be stored. --sequence_example_file is the TFRecord file of SequenceExamples that will be fed to the model. --num_training_steps (optional) is how many update steps to take before exiting the training loop. If left unspecified, the training loop will run until terminated manually. --hparams (optional) can be used to specify hyperparameters other than the defaults. For this example, we specify a custom batch size of 64 instead of the default batch size of 128. Using smaller batch sizes can help reduce memory usage, which can resolve potential out-of-memory issues when training larger models. We'll also use a 2-layer RNN with 64 units each, instead of the default of 2 layers of 128 units each. This will make our model train faster. However, if you have enough compute power, you can try using larger layer sizes for better results. You can also adjust how many previous steps the attention mechanism looks at by changing the attn_length hyperparameter. For this example we leave it at the default value of 40 steps (2.5 bars)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "melody_rnn_train \\\n",
    "--config=attention_rnn \\\n",
    "--run_dir=/tmp/melody_rnn/logdir/run1 \\\n",
    "--sequence_example_file=/tmp/melody_rnn/sequence_examples/training_melodies.tfrecord \\\n",
    "--hparams=\"batch_size=64,rnn_layer_sizes=[64,64]\" \\\n",
    "--num_training_steps=20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optionally run an eval job in parallel. --run_dir, --hparams, and --num_training_steps should all be the same values used for the training job. --sequence_example_file should point to the separate set of eval melodies. Include --eval to make this an eval job, resulting in the model only being evaluated without any of the weights being updated."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "melody_rnn_train \\\n",
    "--config=attention_rnn \\\n",
    "--run_dir=/tmp/melody_rnn/logdir/run1 \\\n",
    "--sequence_example_file=/tmp/melody_rnn/sequence_examples/eval_melodies.tfrecord \\\n",
    "--hparams=\"batch_size=64,rnn_layer_sizes=[64,64]\" \\\n",
    "--num_training_steps=20000 \\\n",
    "--eval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
